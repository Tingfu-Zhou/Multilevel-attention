# Multilevel-attention

Attention mechanism has been widely used in the large language model (LLM). However, whether existing LLMs really have reasoning ability is questionable. Since even the state-of-art LLM still will make simple mistakes. We believe the key to getting real reasoning ability is to circularly use key and value to form a real reasoning link.  Based on this, we introduce the idea of reasoning link and  propose multilevel attention. Several experiments show that our multilevel attention can effectively improve the divergent thinking ability of the transformer based model.
