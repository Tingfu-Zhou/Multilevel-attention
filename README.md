# Multilevel-attention

Attention mechanism is largely inspired by human attention cognitive function. When a person observes an object, he or she will focus on a particular part of the picture that he or she is interested in. For example, when people watch the painting Mona Lisa, they will usually focus on Mona Lisa herself instead of the background environment. This allows people to extract high value information from massive information with limited processing resource. Due to this advantage, attention mechanism is imported in deep learning.  

In deep learning, attention mechanism can match query and key, and output the best matching value. This mechanism has been widely used in the mainstream large language model (LLM). But there is always a double of whether the exiting attention based models really have reasoning ability, or they just mechanically follow reasoning enhance strategies, such as prompt engineering. Moreover, Wei et al. shows that with the increase of scale large language model, LLM will emerge some ability that is not present in small scale of LLM, which leads to the booming computing resource requirement. And there is still a long way to achieve a clear interpretability of this emergence. Even when using the state-of-art large language model such as GPT-4,  its ability to compute the answers in areas like complex reasoning, physics, and mathematics is still limited.

Existing attention mechanisms can be divided into four criteria: the softness of attention, input representations, forms of input feature, and output representations. However, the core idea of attention mechanism is unchanged. That is, using query and key to calculate the attention distribution. Combine the attention distribution with value to get the weighted value. Then compute the context vector using the weighted value. Here are several improvement directions of attention mechanism: sparse attention, low-rank self-attention, linearized attention, prototype, and memory compression, attention with prior. These improvement directions focus on improving existing attention mechanism that does not change the original core idea mentioned above.  

Inspired by the idea of attention mechanism and multilayer perceptron, we assume the key to achieving the real reasoning ability is to recurrently use key and value to form a real reasoning link. Based on this new attention mechanism, we introduce multilevel attention. It is worth noting that multilevel attention is different from recurrent neural network (RNN). In RNN, the hidden state is used in recurrence. In multilevel attention, we recurrently connect key and value to form the reasoning link.
