# Multilevel-attention

Research topic is Attention mechanism; 
Multilevel attention assume the key to achieve the real reasoning ability is to recurrently use key and value to form a real reasoning link. Multilevel attention is different from recurrent neural network (RNN). In RNN, thehidden state is used in recurrence. In multilevel attention, we recurrently connect key and query to form thereasoning link. This article will be
completed by the end of this year (2023).
