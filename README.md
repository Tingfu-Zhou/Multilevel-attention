# Multilevel-attention

Attention mechanism is largely inspired by human attention cognitive function {itti1998model,mnih2014recurrent}. When a person observes a object, he or she will focus on a particular part of the picture that he or she is interested in. For example, when people watch the painting Mona Lisa, they will usually focus on Mona Lisa herself instead of the background environment. This allows people to extract high value information from massive information with limited processing resource. Due to this advantage, attention mechanism is imported in deep learning {mnih2014recurrent}.  

In deep learning, attention mechanism can match query and key, and output the best matching value {bahdanau2014neural}. This mechanism has been widely used in the mainstream large language model (LLM). But here are always has a double that whether the exiting attention based models really have reasoning ability \cite{qiao2022reasoning}, or they just mechanically follow reasoning enhance strategies, such as prompt engineering. Moreover, {wei2022emergent} shows that with the increase of scale large language model, LLM will emerge some ability that is not present in small scale of LLM, which lead to the booming computing resource requirement. And here is still a long way to achieve a clear interpretability of this emergence {qiao2022reasoning}. Even the state-of-art large language model GTP-4 has limit in complex reasoning, physics and mathematics, for example {liu2023summary}.

Existing attention mechanism can be divided into four criteria {niu2021review}: the softness of attention, input representations, forms of input feature and output representations. However, the score idea of attention mechanism is unchanged. That is, using query and key to calculate the attention distribution. Combine the attention distribution with value to get the weighted value. Then compute the context vector using the weighted value. Here are several improvement directions of attention mechanism {lin2022survey}: sparse attention, linearized attention, prototype and memory compression, low-rank self-attention, attention with prior, attention with prior. These improvement directions focus on improving existing attention mechanism that does not change the original score idea mentioned above. 

Inspired by the idea of attention mechanism and multilayer perceptron, we assume the key to achieve the real reasoning ability is to recurrently use key and value to form a real reasoning link. And we call this new attention mechanism as multilevel attention. It is worth to note that multilevel attention is different from recurrent neural network (RNN). In RNN, the hidden state is used in recurrence. In multilevel attention, we recurrently connect key and query to form the reasoning link.
