{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer\n",
    "\n",
    "the source comes from https://zhuanlan.zhihu.com/p/581334630\n",
    "\n",
    "the dataset comes from https://github.com/P3n9W31/transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assume d_model is an even number for convenience\n",
    "        assert d_model % 2 == 0\n",
    "\n",
    "        i_seq = torch.linspace(0, max_seq_len - 1, max_seq_len)\n",
    "        j_seq = torch.linspace(0, d_model - 2, d_model // 2)\n",
    "        pos, two_i = torch.meshgrid(i_seq, j_seq)\n",
    "        pe_2i = torch.sin(pos / 10000**(two_i / d_model))\n",
    "        pe_2i_1 = torch.cos(pos / 10000**(two_i / d_model))\n",
    "        pe = torch.stack((pe_2i, pe_2i_1), 2).reshape(1, max_seq_len, d_model)\n",
    "\n",
    "        self.register_buffer('pe', pe, False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        n, seq_len, d_model = x.shape\n",
    "        pe: torch.Tensor = self.pe\n",
    "        assert seq_len <= pe.shape[1]\n",
    "        assert d_model == pe.shape[2]\n",
    "        rescaled_x = x * d_model**0.5\n",
    "        return rescaled_x + pe[:, 0:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_INF = 1e12\n",
    "\n",
    "def attention(q: torch.Tensor,\n",
    "              k: torch.Tensor,\n",
    "              v: torch.Tensor,\n",
    "              mask: Optional[torch.Tensor] = None):\n",
    "    '''\n",
    "    Note: The dtype of mask must be bool\n",
    "    '''\n",
    "    # q shape: [n, heads, q_len, d_k]\n",
    "    # k shape: [n, heads, k_len, d_k]\n",
    "    # v shape: [n, heads, k_len, d_v]\n",
    "    assert q.shape[-1] == k.shape[-1]\n",
    "    d_k = k.shape[-1]\n",
    "    # tmp shape: [n, heads, q_len, k_len]\n",
    "    tmp = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5\n",
    "    if mask is not None:\n",
    "        tmp.masked_fill_(mask, -MY_INF)\n",
    "    tmp = F.softmax(tmp, -1)\n",
    "    # tmp shape: [n, heads, q_len, d_v]\n",
    "    tmp = torch.matmul(tmp, v)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % heads == 0\n",
    "        # dk == dv\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                q: torch.Tensor,\n",
    "                k: torch.Tensor,\n",
    "                v: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        # batch should be same\n",
    "        assert q.shape[0] == k.shape[0]\n",
    "        assert q.shape[0] == v.shape[0]\n",
    "        # the sequence length of k and v should be aligned\n",
    "        assert k.shape[1] == v.shape[1]\n",
    "\n",
    "        n, q_len = q.shape[0:2]\n",
    "        n, k_len = k.shape[0:2]\n",
    "        q_ = self.q(q).reshape(n, q_len, self.heads, self.d_k).transpose(1, 2)\n",
    "        k_ = self.k(k).reshape(n, k_len, self.heads, self.d_k).transpose(1, 2)\n",
    "        v_ = self.v(v).reshape(n, k_len, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        attention_res = attention(q_, k_, v_, mask)\n",
    "        concat_res = attention_res.transpose(1, 2).reshape(\n",
    "            n, q_len, self.d_model)\n",
    "        concat_res = self.dropout(concat_res)\n",
    "\n",
    "        output = self.out(concat_res)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 heads: int,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(heads, d_model, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
    "        tmp = self.self_attention(x, x, x, src_mask)\n",
    "        tmp = self.dropout1(tmp)\n",
    "        x = self.norm1(x + tmp)\n",
    "        tmp = self.ffn(x)\n",
    "        tmp = self.dropout2(tmp)\n",
    "        x = self.norm2(x + tmp)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 heads: int,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(heads, d_model, dropout)\n",
    "        self.attention = MultiHeadAttention(heads, d_model, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                encoder_kv: torch.Tensor,\n",
    "                dst_mask: Optional[torch.Tensor] = None,\n",
    "                src_dst_mask: Optional[torch.Tensor] = None):\n",
    "        tmp = self.self_attention(x, x, x, dst_mask)\n",
    "        tmp = self.dropout1(tmp)\n",
    "        x = self.norm1(x + tmp)\n",
    "        tmp = self.attention(x, encoder_kv, encoder_kv, src_dst_mask)\n",
    "        tmp = self.dropout2(tmp)\n",
    "        x = self.norm2(x + tmp)\n",
    "        tmp = self.ffn(x)\n",
    "        tmp = self.dropout3(tmp)\n",
    "        x = self.norm3(x + tmp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 pad_idx: int,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 n_layers: int,\n",
    "                 heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_len: int = 120):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(EncoderLayer(heads, d_model, d_ff, dropout))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 pad_idx: int,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 n_layers: int,\n",
    "                 heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_len: int = 120):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(DecoderLayer(heads, d_model, d_ff, dropout))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                encoder_kv,\n",
    "                dst_mask: Optional[torch.Tensor] = None,\n",
    "                src_dst_mask: Optional[torch.Tensor] = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_kv, dst_mask, src_dst_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size: int,\n",
    "                 dst_vocab_size: int,\n",
    "                 pad_idx: int,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 n_layers: int,\n",
    "                 heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, pad_idx, d_model, d_ff,\n",
    "                               n_layers, heads, dropout, max_seq_len)\n",
    "        self.decoder = Decoder(dst_vocab_size, pad_idx, d_model, d_ff,\n",
    "                               n_layers, heads, dropout, max_seq_len)\n",
    "        self.pad_idx = pad_idx\n",
    "        self.output_layer = nn.Linear(d_model, dst_vocab_size)\n",
    "\n",
    "    def generate_mask(self,\n",
    "                      q_pad: torch.Tensor,\n",
    "                      k_pad: torch.Tensor,\n",
    "                      with_left_mask: bool = False):\n",
    "        # q_pad shape: [n, q_len]\n",
    "        # k_pad shape: [n, k_len]\n",
    "        # q_pad k_pad dtype: bool\n",
    "        assert q_pad.device == k_pad.device\n",
    "        n, q_len = q_pad.shape\n",
    "        n, k_len = k_pad.shape\n",
    "\n",
    "        mask_shape = (n, 1, q_len, k_len)\n",
    "        if with_left_mask:\n",
    "            mask = 1 - torch.tril(torch.ones(mask_shape))\n",
    "        else:\n",
    "            mask = torch.zeros(mask_shape)\n",
    "        mask = mask.to(q_pad.device)\n",
    "        for i in range(n):\n",
    "            mask[i, :, q_pad[i], :] = 1\n",
    "            mask[i, :, :, k_pad[i]] = 1\n",
    "        mask = mask.to(torch.bool)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        src_pad_mask = x == self.pad_idx\n",
    "        dst_pad_mask = y == self.pad_idx\n",
    "        src_mask = self.generate_mask(src_pad_mask, src_pad_mask, False)\n",
    "        dst_mask = self.generate_mask(dst_pad_mask, dst_pad_mask, True)\n",
    "        src_dst_mask = self.generate_mask(dst_pad_mask, src_pad_mask, False)\n",
    "        encoder_kv = self.encoder(x, src_mask)\n",
    "        res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)\n",
    "        res = self.output_layer(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install cn.txt ...\n",
      "Install successfully.\n",
      "Install en.txt ...\n",
      "Install successfully.\n",
      "Install cn.txt.vocab.tsv ...\n",
      "Install successfully.\n",
      "Install en.txt.vocab.tsv ...\n",
      "Install successfully.\n"
     ]
    }
   ],
   "source": [
    "# Modify from\n",
    "# https://github.com/P3n9W31/transformer-pytorch/master/data_load.py\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import regex\n",
    "import requests\n",
    "\n",
    "# Words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "min_cnt = 0\n",
    "# Maximum number of words in a sentence.\n",
    "maxlen = 50\n",
    "\n",
    "source_train = 'dldemos/Transformer/data/cn.txt'\n",
    "target_train = 'dldemos/Transformer/data/en.txt'\n",
    "source_test = 'dldemos/Transformer/data/cn.test.txt'\n",
    "target_test = 'dldemos/Transformer/data/en.test.txt'\n",
    "\n",
    "\n",
    "def load_vocab(language):\n",
    "    assert language in ['cn', 'en']\n",
    "    vocab = [\n",
    "        line.split()[0] for line in codecs.open(\n",
    "            'dldemos/Transformer/data/{}.txt.vocab.tsv'.format(language), 'r',\n",
    "            'utf-8').read().splitlines() if int(line.split()[1]) >= min_cnt\n",
    "    ]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def load_cn_vocab():\n",
    "    word2idx, idx2word = load_vocab('cn')\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def load_en_vocab():\n",
    "    word2idx, idx2word = load_vocab('en')\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def create_data(source_sents, target_sents):\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "\n",
    "    # Index\n",
    "    x_list, y_list, Sources, Targets = [], [], [], []\n",
    "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
    "        x = [\n",
    "            cn2idx.get(word, 1)\n",
    "            for word in ('<S> ' + source_sent + ' </S>').split()\n",
    "        ]  # 1: OOV, </S>: End of Text\n",
    "        y = [\n",
    "            en2idx.get(word, 1)\n",
    "            for word in ('<S> ' + target_sent + ' </S>').split()\n",
    "        ]\n",
    "        if max(len(x), len(y)) <= maxlen:\n",
    "            x_list.append(np.array(x))\n",
    "            y_list.append(np.array(y))\n",
    "            Sources.append(source_sent)\n",
    "            Targets.append(target_sent)\n",
    "\n",
    "    # Pad\n",
    "    X = np.zeros([len(x_list), maxlen], np.int32)\n",
    "    Y = np.zeros([len(y_list), maxlen], np.int32)\n",
    "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
    "        X[i] = np.lib.pad(x, [0, maxlen - len(x)],\n",
    "                          'constant',\n",
    "                          constant_values=(0, 0))\n",
    "        Y[i] = np.lib.pad(y, [0, maxlen - len(y)],\n",
    "                          'constant',\n",
    "                          constant_values=(0, 0))\n",
    "\n",
    "    return X, Y, Sources, Targets\n",
    "\n",
    "\n",
    "def load_data(data_type):\n",
    "    if data_type == 'train':\n",
    "        source, target = source_train, target_train\n",
    "    elif data_type == 'test':\n",
    "        source, target = source_test, target_test\n",
    "    assert data_type in ['train', 'test']\n",
    "    cn_sents = [\n",
    "        regex.sub(\"[^\\s\\p{L}']\", '', line)  # noqa W605\n",
    "        for line in codecs.open(source, 'r', 'utf-8').read().split('\\n')\n",
    "        if line and line[0] != '<'\n",
    "    ]\n",
    "    en_sents = [\n",
    "        regex.sub(\"[^\\s\\p{L}']\", '', line)  # noqa W605\n",
    "        for line in codecs.open(target, 'r', 'utf-8').read().split('\\n')\n",
    "        if line and line[0] != '<'\n",
    "    ]\n",
    "\n",
    "    X, Y, Sources, Targets = create_data(cn_sents, en_sents)\n",
    "    return X, Y, Sources, Targets\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    X, Y, _, _ = load_data('train')\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    X, Y, _, _ = load_data('test')\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_batch_indices(total_length, batch_size):\n",
    "    assert (batch_size <=\n",
    "            total_length), ('Batch size is large than total data length.'\n",
    "                            'Check your data or change batch size.')\n",
    "    current_index = 0\n",
    "    indexs = [i for i in range(total_length)]\n",
    "    random.shuffle(indexs)\n",
    "    while 1:\n",
    "        if current_index + batch_size >= total_length:\n",
    "            break\n",
    "        current_index += batch_size\n",
    "        yield indexs[current_index:current_index + batch_size], current_index\n",
    "\n",
    "\n",
    "def idx_to_sentence(arr, vocab, insert_space=False):\n",
    "    res = ''\n",
    "    first_word = True\n",
    "    for id in arr:\n",
    "        word = vocab[id.item()]\n",
    "\n",
    "        if insert_space and not first_word:\n",
    "            res += ' '\n",
    "        first_word = False\n",
    "\n",
    "        res += word\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def download(url, dir, name=None):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    if name is None:\n",
    "        name = url.split('/')[-1]\n",
    "    path = os.path.join(dir, name)\n",
    "    if not os.path.exists(path):\n",
    "        print(f'Install {name} ...')\n",
    "        open(path, 'wb').write(requests.get(url).content)\n",
    "        print('Install successfully.')\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    data_dir = 'dldemos/Transformer/data'\n",
    "    urls = [('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "             'master/corpora/cn.txt'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "             'master/corpora/en.txt'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "             'master/preprocessed/cn.txt.vocab.tsv'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "             'master/preprocessed/en.txt.vocab.tsv')]\n",
    "    for url in urls:\n",
    "        download(url, data_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tingf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000 00:01 loss: 9.625300407409668 acc: 0.0\n",
      "00000100 00:23 loss: 7.104404926300049 acc: 0.09090909361839294\n",
      "00000200 00:46 loss: 6.803500652313232 acc: 0.1268954575061798\n",
      "00000300 01:09 loss: 6.544301509857178 acc: 0.1341557502746582\n",
      "00000400 01:32 loss: 6.181447505950928 acc: 0.16018307209014893\n",
      "00000500 01:54 loss: 6.2769975662231445 acc: 0.1577707976102829\n",
      "00000600 02:17 loss: 5.9968485832214355 acc: 0.17934782803058624\n",
      "00000700 02:40 loss: 5.949433326721191 acc: 0.18789808452129364\n",
      "00000800 03:03 loss: 5.995901584625244 acc: 0.16562500596046448\n",
      "00000900 03:25 loss: 5.480592250823975 acc: 0.21389107406139374\n",
      "00001000 03:48 loss: 5.228718280792236 acc: 0.22601109743118286\n",
      "00001100 04:11 loss: 5.2937188148498535 acc: 0.21279875934123993\n",
      "00001200 04:34 loss: 5.068534851074219 acc: 0.239215686917305\n",
      "00001300 04:57 loss: 4.968890190124512 acc: 0.2370723932981491\n",
      "00001400 05:20 loss: 4.785625457763672 acc: 0.25275591015815735\n",
      "00001500 05:43 loss: 4.7662177085876465 acc: 0.2566719055175781\n",
      "00001600 06:06 loss: 4.54657506942749 acc: 0.27952754497528076\n",
      "00001700 06:28 loss: 4.311945915222168 acc: 0.30492570996284485\n",
      "00001800 06:51 loss: 4.36310338973999 acc: 0.3041117191314697\n",
      "00001900 07:14 loss: 4.3798112869262695 acc: 0.289741575717926\n",
      "00002000 07:36 loss: 3.85422945022583 acc: 0.34761905670166016\n",
      "00002100 07:59 loss: 3.9817700386047363 acc: 0.3112480640411377\n",
      "00002200 08:22 loss: 3.715763807296753 acc: 0.32763975858688354\n",
      "00002300 08:45 loss: 3.58394193649292 acc: 0.35211268067359924\n",
      "00002400 09:08 loss: 3.5069491863250732 acc: 0.3592677414417267\n",
      "00002500 09:31 loss: 3.3696091175079346 acc: 0.3750978708267212\n",
      "00002600 09:54 loss: 3.205556631088257 acc: 0.400778204202652\n",
      "00002700 10:17 loss: 3.2076525688171387 acc: 0.3987441062927246\n",
      "00002800 10:39 loss: 2.8705832958221436 acc: 0.4473889470100403\n",
      "00002900 11:02 loss: 2.83239483833313 acc: 0.4379391074180603\n",
      "00003000 11:25 loss: 2.720292329788208 acc: 0.46322378516197205\n",
      "00003100 11:49 loss: 2.528949737548828 acc: 0.4892942011356354\n",
      "00003200 12:13 loss: 2.396054267883301 acc: 0.5104570388793945\n",
      "00003300 12:36 loss: 2.3714354038238525 acc: 0.5082612037658691\n",
      "00003400 12:59 loss: 2.092193841934204 acc: 0.551993727684021\n",
      "00003500 13:22 loss: 2.195706605911255 acc: 0.5377358198165894\n",
      "00003600 13:45 loss: 2.106813430786133 acc: 0.5640432238578796\n",
      "00003700 14:08 loss: 2.0865135192871094 acc: 0.5511810779571533\n",
      "00003800 14:30 loss: 1.9566395282745361 acc: 0.6020249128341675\n",
      "00003900 14:53 loss: 1.803696870803833 acc: 0.6317859888076782\n",
      "00004000 15:16 loss: 1.7728503942489624 acc: 0.6244056820869446\n",
      "00004100 15:39 loss: 1.6826423406600952 acc: 0.6394081115722656\n",
      "00004200 16:02 loss: 1.5927410125732422 acc: 0.6533957719802856\n",
      "00004300 35:14 loss: 1.3914575576782227 acc: 0.6864274740219116\n",
      "00004400 35:38 loss: 1.3050963878631592 acc: 0.7113003134727478\n",
      "00004500 36:00 loss: 1.274338960647583 acc: 0.7050583362579346\n",
      "00004600 36:23 loss: 1.1691888570785522 acc: 0.7572662830352783\n",
      "00004700 36:46 loss: 1.1349525451660156 acc: 0.7677165269851685\n",
      "00004800 37:08 loss: 1.046179175376892 acc: 0.7822014093399048\n",
      "00004900 37:31 loss: 0.9498416781425476 acc: 0.788536012172699\n",
      "00005000 37:53 loss: 0.87408047914505 acc: 0.7995319962501526\n",
      "00005100 38:16 loss: 0.7943695783615112 acc: 0.8442367315292358\n",
      "00005200 38:39 loss: 0.6807563900947571 acc: 0.8777258396148682\n",
      "00005300 39:01 loss: 0.7254120111465454 acc: 0.8523543477058411\n",
      "00005400 39:24 loss: 0.7268040776252747 acc: 0.8392036557197571\n",
      "00005500 39:47 loss: 0.6711929440498352 acc: 0.8598130941390991\n",
      "00005600 40:10 loss: 0.7027729153633118 acc: 0.8390357494354248\n",
      "00005700 40:33 loss: 0.6098051071166992 acc: 0.8619084358215332\n",
      "00005800 40:56 loss: 0.5653130412101746 acc: 0.8825816512107849\n",
      "00005900 41:18 loss: 0.5427618026733398 acc: 0.8892355561256409\n",
      "00006000 41:41 loss: 0.49336177110671997 acc: 0.8933333158493042\n",
      "00006100 42:03 loss: 0.4918259084224701 acc: 0.8983451724052429\n",
      "00006200 42:26 loss: 0.454914927482605 acc: 0.9069585800170898\n",
      "00006300 42:50 loss: 0.4096639156341553 acc: 0.911856472492218\n",
      "Model saved to dldemos/Transformer/model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "# Config\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "heads = 8\n",
    "dropout_rate = 0.2\n",
    "n_epochs = 60\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda'\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    # X: en\n",
    "    # Y: cn\n",
    "    Y, X = load_train_data()\n",
    "\n",
    "    print_interval = 100\n",
    "\n",
    "    model = Transformer(len(en2idx), len(cn2idx), PAD_ID, d_model, d_ff,\n",
    "                        n_layers, heads, dropout_rate, maxlen)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    citerion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "    tic = time.time()\n",
    "    cnter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for index, _ in get_batch_indices(len(X), batch_size):\n",
    "            x_batch = torch.LongTensor(X[index]).to(device)\n",
    "            y_batch = torch.LongTensor(Y[index]).to(device)\n",
    "            y_input = y_batch[:, :-1]\n",
    "            y_label = y_batch[:, 1:]\n",
    "            y_hat = model(x_batch, y_input)\n",
    "\n",
    "            y_label_mask = y_label != PAD_ID\n",
    "            preds = torch.argmax(y_hat, -1)\n",
    "            correct = preds == y_label\n",
    "            acc = torch.sum(y_label_mask * correct) / torch.sum(y_label_mask)\n",
    "\n",
    "            n, seq_len = y_label.shape\n",
    "            y_hat = torch.reshape(y_hat, (n * seq_len, -1))\n",
    "            y_label = torch.reshape(y_label, (n * seq_len, ))\n",
    "            loss = citerion(y_hat, y_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if cnter % print_interval == 0:\n",
    "                toc = time.time()\n",
    "                interval = toc - tic\n",
    "                minutes = int(interval // 60)\n",
    "                seconds = int(interval % 60)\n",
    "                print(f'{cnter:08d} {minutes:02d}:{seconds:02d}'\n",
    "                      f' loss: {loss.item()} acc: {acc.item()}')\n",
    "            cnter += 1\n",
    "\n",
    "    model_path = 'dldemos/Transformer/model.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we should protect environment\n",
      "<S> 要 环境 保护 环境 保护 环境 环境 保护 环境 环境 保护 环境 环境 保护 环境 环境 保护 环境 环境 保护 环境 环境 环境 保护 环境 环境 环境 环境 环境 环境 </S> 环境 </S> 吗 </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Config\n",
    "batch_size = 1\n",
    "lr = 0.0001\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "heads = 8\n",
    "dropout_rate = 0.2\n",
    "n_epochs = 60\n",
    "\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda'\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "\n",
    "    model = Transformer(len(en2idx), len(cn2idx), 0, d_model, d_ff, n_layers,\n",
    "                        heads, dropout_rate, maxlen)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_path = 'dldemos/Transformer/model.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    my_input = ['we', \"should\", \"protect\", \"environment\"]\n",
    "    x_batch = torch.LongTensor([[en2idx[x] for x in my_input]]).to(device)\n",
    "\n",
    "    cn_sentence = idx_to_sentence(x_batch[0], idx2en, True)\n",
    "    print(cn_sentence)\n",
    "\n",
    "    y_input = torch.ones(batch_size, maxlen,\n",
    "                         dtype=torch.long).to(device) * PAD_ID\n",
    "    y_input[0] = en2idx['<S>']\n",
    "    # y_input = y_batch\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, y_input.shape[1]):\n",
    "            y_hat = model(x_batch, y_input)\n",
    "            for j in range(batch_size):\n",
    "                y_input[j, i] = torch.argmax(y_hat[j, i - 1])\n",
    "    output_sentence = idx_to_sentence(y_input[0], idx2cn, True)\n",
    "    print(output_sentence)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
